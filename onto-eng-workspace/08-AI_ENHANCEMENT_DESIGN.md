# ğŸ¤– AIå¢å¼ºåŠŸèƒ½ - è¯¦ç»†è®¾è®¡

## ğŸ“‹ æ¨¡å—æ¦‚è¿°

æœ¬æ¨¡å—æä¾›AIåŸç”Ÿçš„æ™ºèƒ½è¾…åŠ©èƒ½åŠ›ï¼ŒåŒ…æ‹¬çŸ¥è¯†å›¾è°±åµŒå…¥ã€è‡ªç„¶è¯­è¨€æŸ¥è¯¢ã€æ™ºèƒ½æ¨èå’Œè‡ªåŠ¨çŸ¥è¯†æŠ½å–ã€‚

**æ ¸å¿ƒç›®æ ‡**:
- å›¾åµŒå…¥è®­ç»ƒå’Œåº”ç”¨
- NLè½¬SPARQLè‡ªç„¶è¯­è¨€æŸ¥è¯¢
- æ™ºèƒ½æ¨èå’Œå…³ç³»é¢„æµ‹
- è‡ªåŠ¨å®ä½“å…³ç³»æŠ½å–

---

## ğŸ¯ AIæœåŠ¡æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Reactå‰ç«¯                    â”‚
â”‚  - NLæŸ¥è¯¢ç•Œé¢                        â”‚
â”‚  - æ¨èå±•ç¤º                          â”‚
â”‚  - å‘é‡å¯è§†åŒ–                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚ REST API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Express APIç½‘å…³                â”‚
â”‚   - è¯·æ±‚è·¯ç”±                         â”‚
â”‚   - è®¤è¯æˆæƒ                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Python AIæœåŠ¡ (Flask/FastAPI) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¨¡å—1: å›¾åµŒå…¥                       â”‚
â”‚  - Node2Vec                          â”‚
â”‚  - TransE                            â”‚
â”‚  - å‘é‡å­˜å‚¨(FAISS)                   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¨¡å—2: è‡ªç„¶è¯­è¨€å¤„ç†                 â”‚
â”‚  - LLMé›†æˆ(OpenAI/æœ¬åœ°)              â”‚
â”‚  - NLâ†’SPARQL                         â”‚
â”‚  - é—®ç­”ç³»ç»Ÿ                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¨¡å—3: æ¨èç³»ç»Ÿ                     â”‚
â”‚  - ååŒè¿‡æ»¤                          â”‚
â”‚  - å…³ç³»é¢„æµ‹                          â”‚
â”‚  - çŸ¥è¯†è¡¥å…¨                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æ¨¡å—4: çŸ¥è¯†æŠ½å–                     â”‚
â”‚  - NER(å‘½åå®ä½“è¯†åˆ«)                 â”‚
â”‚  - RE(å…³ç³»æŠ½å–)                      â”‚
â”‚  - å®ä½“é“¾æ¥                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1. çŸ¥è¯†å›¾è°±åµŒå…¥

### 1.1 Python AIæœåŠ¡æ­å»º

```python
# ai_service/app.py
from flask import Flask, request, jsonify
from flask_cors import CORS
import numpy as np

app = Flask(__name__)
CORS(app)

# å…¨å±€æ¨¡å‹ç®¡ç†å™¨
from models import EmbeddingManager

embedding_manager = EmbeddingManager()

@app.route('/api/ai/health', methods=['GET'])
def health_check():
    return jsonify({'status': 'ok', 'service': 'AI Service'})

@app.route('/api/ai/embedding/train', methods=['POST'])
def train_embedding():
    """è®­ç»ƒå›¾åµŒå…¥æ¨¡å‹"""
    data = request.json
    graph_data = data['graph']
    algorithm = data.get('algorithm', 'node2vec')
    params = data.get('params', {})
    
    result = embedding_manager.train(
        graph_data=graph_data,
        algorithm=algorithm,
        **params
    )
    
    return jsonify({
        'success': True,
        'model_id': result['model_id'],
        'metrics': result['metrics']
    })

@app.route('/api/ai/embedding/query', methods=['POST'])
def query_similar():
    """æŸ¥è¯¢ç›¸ä¼¼èŠ‚ç‚¹"""
    data = request.json
    node_id = data['node_id']
    top_k = data.get('top_k', 10)
    
    similar = embedding_manager.find_similar(node_id, top_k)
    
    return jsonify({
        'success': True,
        'similar_nodes': similar
    })

if __name__ == '__main__':
    app.run(host='0.0.0.0', port=5000, debug=True)
```

### 1.2 Node2Vecå®ç°

```python
# ai_service/models/node2vec_model.py
import numpy as np
from gensim.models import Word2Vec
from node2vec import Node2Vec as N2V
import networkx as nx

class Node2VecModel:
    def __init__(self, dimensions=128, walk_length=80, num_walks=10):
        self.dimensions = dimensions
        self.walk_length = walk_length
        self.num_walks = num_walks
        self.model = None
        self.graph = None
    
    def train(self, graph_data):
        """
        è®­ç»ƒNode2Vecæ¨¡å‹
        
        Args:
            graph_data: { nodes: [...], edges: [...] }
        """
        # æ„å»ºNetworkXå›¾
        G = nx.DiGraph()
        
        for node in graph_data['nodes']:
            G.add_node(node['id'], **node['data'])
        
        for edge in graph_data['edges']:
            G.add_edge(edge['source'], edge['target'], **edge['data'])
        
        self.graph = G
        
        # åˆå§‹åŒ–Node2Vec
        node2vec = N2V(
            G,
            dimensions=self.dimensions,
            walk_length=self.walk_length,
            num_walks=self.num_walks,
            workers=4
        )
        
        # ç”Ÿæˆéšæœºæ¸¸èµ°
        model = node2vec.fit(
            window=10,
            min_count=1,
            batch_words=4
        )
        
        self.model = model
        
        return {
            'node_count': G.number_of_nodes(),
            'edge_count': G.number_of_edges(),
            'dimensions': self.dimensions
        }
    
    def get_embedding(self, node_id):
        """è·å–èŠ‚ç‚¹åµŒå…¥å‘é‡"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        try:
            return self.model.wv[node_id]
        except KeyError:
            return None
    
    def find_similar(self, node_id, top_k=10):
        """æŸ¥æ‰¾æœ€ç›¸ä¼¼çš„èŠ‚ç‚¹"""
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        try:
            similar = self.model.wv.most_similar(node_id, topn=top_k)
            return [
                {
                    'node_id': node,
                    'similarity': float(score)
                }
                for node, score in similar
            ]
        except KeyError:
            return []
    
    def visualize_embeddings(self, method='tsne'):
        """å¯è§†åŒ–åµŒå…¥ï¼ˆé™ç»´åˆ°2Dï¼‰"""
        from sklearn.manifold import TSNE
        from umap import UMAP
        
        if self.model is None:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")
        
        # è·å–æ‰€æœ‰åµŒå…¥
        node_ids = list(self.model.wv.index_to_key)
        embeddings = np.array([self.model.wv[n] for n in node_ids])
        
        # é™ç»´
        if method == 'tsne':
            reducer = TSNE(n_components=2, random_state=42)
        else:
            reducer = UMAP(n_components=2, random_state=42)
        
        embeddings_2d = reducer.fit_transform(embeddings)
        
        return {
            'nodes': node_ids,
            'coordinates': embeddings_2d.tolist()
        }
```

### 1.3 å‘é‡å­˜å‚¨(FAISS)

```python
# ai_service/models/vector_store.py
import faiss
import numpy as np
import pickle

class VectorStore:
    def __init__(self, dimension=128):
        self.dimension = dimension
        self.index = faiss.IndexFlatL2(dimension)
        self.id_map = []  # node_idåˆ—è¡¨
    
    def add_vectors(self, node_ids, vectors):
        """æ·»åŠ å‘é‡åˆ°ç´¢å¼•"""
        vectors_np = np.array(vectors).astype('float32')
        self.index.add(vectors_np)
        self.id_map.extend(node_ids)
    
    def search(self, query_vector, k=10):
        """æœç´¢æœ€ç›¸ä¼¼çš„å‘é‡"""
        query_np = np.array([query_vector]).astype('float32')
        distances, indices = self.index.search(query_np, k)
        
        results = []
        for dist, idx in zip(distances[0], indices[0]):
            if idx < len(self.id_map):
                results.append({
                    'node_id': self.id_map[idx],
                    'distance': float(dist),
                    'similarity': float(1 / (1 + dist))  # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
                })
        
        return results
    
    def save(self, filepath):
        """ä¿å­˜ç´¢å¼•"""
        faiss.write_index(self.index, f"{filepath}.index")
        with open(f"{filepath}.map", 'wb') as f:
            pickle.dump(self.id_map, f)
    
    def load(self, filepath):
        """åŠ è½½ç´¢å¼•"""
        self.index = faiss.read_index(f"{filepath}.index")
        with open(f"{filepath}.map", 'rb') as f:
            self.id_map = pickle.load(f)
```

---

## 2. è‡ªç„¶è¯­è¨€æŸ¥è¯¢

### 2.1 NLè½¬SPARQL

```python
# ai_service/nl_query/nl_to_sparql.py
from langchain.llms import OpenAI
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain

class NLToSPARQL:
    def __init__(self, api_key=None):
        self.llm = OpenAI(
            api_key=api_key,
            model_name="gpt-4",
            temperature=0
        )
        
        self.prompt = PromptTemplate(
            input_variables=["schema", "question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªSPARQLæŸ¥è¯¢ä¸“å®¶ã€‚æ ¹æ®ç»™å®šçš„æœ¬ä½“Schemaå’Œç”¨æˆ·é—®é¢˜ï¼Œç”Ÿæˆå¯¹åº”çš„SPARQLæŸ¥è¯¢ã€‚

Schemaä¿¡æ¯:
{schema}

ç”¨æˆ·é—®é¢˜: {question}

è¯·ç”ŸæˆSPARQLæŸ¥è¯¢ï¼ˆåªè¿”å›æŸ¥è¯¢è¯­å¥ï¼Œä¸è¦è§£é‡Šï¼‰:
"""
        )
        
        self.chain = LLMChain(llm=self.llm, prompt=self.prompt)
    
    def convert(self, question, schema):
        """å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸ºSPARQLæŸ¥è¯¢"""
        
        # æ ¼å¼åŒ–Schemaä¿¡æ¯
        schema_text = self._format_schema(schema)
        
        # ç”ŸæˆSPARQL
        sparql = self.chain.run(
            schema=schema_text,
            question=question
        )
        
        # æ¸…ç†å’ŒéªŒè¯
        sparql = self._clean_sparql(sparql)
        
        return {
            'sparql': sparql,
            'question': question
        }
    
    def _format_schema(self, schema):
        """æ ¼å¼åŒ–Schemaä¸ºæ–‡æœ¬"""
        lines = []
        
        # å®ä½“ç±»å‹
        lines.append("å®ä½“ç±»å‹:")
        for type_id, type_def in schema.get('entityTypes', {}).items():
            lines.append(f"  - {type_def['label']} ({type_id})")
            if type_def.get('properties'):
                for prop, prop_def in type_def['properties'].items():
                    lines.append(f"    å±æ€§: {prop} ({prop_def['type']})")
        
        # å…³ç³»ç±»å‹
        lines.append("\nå…³ç³»ç±»å‹:")
        for rel_id, rel_def in schema.get('relationTypes', {}).items():
            lines.append(f"  - {rel_def['label']} ({rel_id})")
            lines.append(f"    ä»: {rel_def.get('from', [])} åˆ°: {rel_def.get('to', [])}")
        
        return "\n".join(lines)
    
    def _clean_sparql(self, sparql):
        """æ¸…ç†SPARQLæŸ¥è¯¢"""
        # ç§»é™¤markdownä»£ç å—æ ‡è®°
        sparql = sparql.replace('```sparql', '').replace('```', '')
        # å»é™¤å‰åç©ºç™½
        sparql = sparql.strip()
        return sparql
```

### 2.2 é—®ç­”ç³»ç»Ÿ

```python
# ai_service/nl_query/qa_system.py
from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory

class QASystem:
    def __init__(self, llm, sparql_executor):
        self.llm = llm
        self.sparql_executor = sparql_executor
        self.memory = ConversationBufferMemory(
            memory_key="chat_history",
            return_messages=True
        )
    
    def answer(self, question, context=None):
        """å›ç­”é—®é¢˜"""
        
        # 1. å°†é—®é¢˜è½¬æ¢ä¸ºSPARQL
        sparql_result = self.nl_to_sparql.convert(question, context['schema'])
        sparql = sparql_result['sparql']
        
        # 2. æ‰§è¡ŒSPARQLæŸ¥è¯¢
        query_result = self.sparql_executor.execute(sparql)
        
        # 3. ç”¨LLMç”Ÿæˆè‡ªç„¶è¯­è¨€ç­”æ¡ˆ
        answer_prompt = f"""
åŸºäºä»¥ä¸‹æŸ¥è¯¢ç»“æœå›ç­”ç”¨æˆ·é—®é¢˜:

é—®é¢˜: {question}
æŸ¥è¯¢ç»“æœ: {query_result}

è¯·ç”¨è‡ªç„¶è¯­è¨€å›ç­”:
"""
        answer = self.llm(answer_prompt)
        
        # 4. ä¿å­˜åˆ°å¯¹è¯å†å²
        self.memory.save_context(
            {"input": question},
            {"output": answer}
        )
        
        return {
            'answer': answer,
            'sparql': sparql,
            'results': query_result,
            'confidence': 0.8
        }
```

---

## 3. æ™ºèƒ½æ¨è

### 3.1 å…³ç³»é¢„æµ‹

```python
# ai_service/recommendation/relation_predictor.py
import torch
import torch.nn as nn
from torch_geometric.nn import GCNConv

class RelationPredictor(nn.Module):
    """åŸºäºGNNçš„å…³ç³»é¢„æµ‹æ¨¡å‹"""
    
    def __init__(self, num_nodes, num_relations, embedding_dim=64):
        super().__init__()
        
        self.num_nodes = num_nodes
        self.num_relations = num_relations
        self.embedding_dim = embedding_dim
        
        # èŠ‚ç‚¹åµŒå…¥
        self.node_embedding = nn.Embedding(num_nodes, embedding_dim)
        
        # GCNå±‚
        self.conv1 = GCNConv(embedding_dim, embedding_dim)
        self.conv2 = GCNConv(embedding_dim, embedding_dim)
        
        # å…³ç³»åˆ†ç±»å™¨
        self.relation_classifier = nn.Sequential(
            nn.Linear(embedding_dim * 2, 128),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(128, num_relations)
        )
    
    def forward(self, edge_index, head, tail):
        # è·å–èŠ‚ç‚¹åµŒå…¥
        x = self.node_embedding.weight
        
        # GCNä¼ æ’­
        x = self.conv1(x, edge_index)
        x = torch.relu(x)
        x = self.conv2(x, edge_index)
        
        # è·å–å¤´å°¾èŠ‚ç‚¹è¡¨ç¤º
        head_embed = x[head]
        tail_embed = x[tail]
        
        # æ‹¼æ¥å¹¶åˆ†ç±»
        edge_embed = torch.cat([head_embed, tail_embed], dim=1)
        relation_scores = self.relation_classifier(edge_embed)
        
        return relation_scores
    
    def predict(self, source_id, target_id, top_k=3):
        """é¢„æµ‹ä¸¤ä¸ªèŠ‚ç‚¹é—´æœ€å¯èƒ½çš„å…³ç³»"""
        self.eval()
        
        with torch.no_grad():
            scores = self.forward(
                edge_index=self.graph_structure,
                head=torch.tensor([source_id]),
                tail=torch.tensor([target_id])
            )
            
            # Top-Kå…³ç³»
            top_scores, top_indices = torch.topk(scores, k=top_k)
            
            return [
                {
                    'relation_id': int(idx),
                    'confidence': float(score)
                }
                for score, idx in zip(top_scores[0], top_indices[0])
            ]
```

### 3.2 çŸ¥è¯†è¡¥å…¨

```python
# ai_service/recommendation/knowledge_completion.py
class KnowledgeCompletion:
    def __init__(self, embedding_model, relation_predictor):
        self.embedding_model = embedding_model
        self.relation_predictor = relation_predictor
    
    def suggest_missing_edges(self, threshold=0.7):
        """å»ºè®®ç¼ºå¤±çš„è¾¹"""
        suggestions = []
        
        # éå†æ‰€æœ‰èŠ‚ç‚¹å¯¹
        nodes = self.get_all_nodes()
        
        for i, source in enumerate(nodes):
            for target in nodes[i+1:]:
                # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨è¾¹
                if self.edge_exists(source, target):
                    continue
                
                # é¢„æµ‹å…³ç³»
                predictions = self.relation_predictor.predict(source, target)
                
                # å¦‚æœç½®ä¿¡åº¦é«˜ï¼ŒåŠ å…¥å»ºè®®
                for pred in predictions:
                    if pred['confidence'] > threshold:
                        suggestions.append({
                            'source': source,
                            'target': target,
                            'relation': pred['relation_id'],
                            'confidence': pred['confidence'],
                            'reason': 'PREDICTION'
                        })
        
        # æŒ‰ç½®ä¿¡åº¦æ’åº
        suggestions.sort(key=lambda x: x['confidence'], reverse=True)
        
        return suggestions[:100]  # è¿”å›Top 100
```

---

## 4. è‡ªåŠ¨çŸ¥è¯†æŠ½å–

### 4.1 å‘½åå®ä½“è¯†åˆ«(NER)

```python
# ai_service/extraction/ner_extractor.py
from transformers import pipeline
import spacy

class NERExtractor:
    def __init__(self):
        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        self.nlp = spacy.load("zh_core_web_sm")  # ä¸­æ–‡
        self.ner_pipeline = pipeline(
            "ner",
            model="bert-base-chinese",
            aggregation_strategy="simple"
        )
    
    def extract_entities(self, text):
        """ä»æ–‡æœ¬ä¸­æå–å®ä½“"""
        
        # ä½¿ç”¨spaCy
        doc = self.nlp(text)
        spacy_entities = [
            {
                'text': ent.text,
                'label': ent.label_,
                'start': ent.start_char,
                'end': ent.end_char
            }
            for ent in doc.ents
        ]
        
        # ä½¿ç”¨Transformersï¼ˆæ›´å‡†ç¡®ï¼‰
        bert_entities = self.ner_pipeline(text)
        
        # åˆå¹¶ç»“æœ
        entities = self._merge_entities(spacy_entities, bert_entities)
        
        return entities
    
    def _merge_entities(self, entities1, entities2):
        """åˆå¹¶ä¸¤ä¸ªNERç»“æœ"""
        # å»é‡å’Œåˆå¹¶é€»è¾‘
        merged = {}
        
        for ent in entities1 + entities2:
            key = (ent['start'], ent['end'])
            if key not in merged:
                merged[key] = ent
        
        return list(merged.values())
```

### 4.2 å…³ç³»æŠ½å–(RE)

```python
# ai_service/extraction/relation_extractor.py
from transformers import AutoTokenizer, AutoModelForSequenceClassification
import torch

class RelationExtractor:
    def __init__(self, model_name="bert-base-chinese"):
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)
        self.model = AutoModelForSequenceClassification.from_pretrained(
            model_name,
            num_labels=10  # å…³ç³»ç±»å‹æ•°é‡
        )
        
        self.relation_types = [
            'depends_on', 'implements', 'contains', 'related_to',
            'parent_of', 'similar_to', 'part_of', 'assigned_to',
            'created_by', 'none'
        ]
    
    def extract_relations(self, text, entities):
        """ä»æ–‡æœ¬å’Œå®ä½“ä¸­æŠ½å–å…³ç³»"""
        relations = []
        
        # éå†å®ä½“å¯¹
        for i, ent1 in enumerate(entities):
            for ent2 in entities[i+1:]:
                # æ„é€ è¾“å…¥
                input_text = f"{ent1['text']} [SEP] {ent2['text']} [SEP] {text}"
                
                # é¢„æµ‹å…³ç³»
                relation = self._predict_relation(input_text)
                
                if relation['type'] != 'none' and relation['confidence'] > 0.7:
                    relations.append({
                        'source': ent1['text'],
                        'target': ent2['text'],
                        'relation': relation['type'],
                        'confidence': relation['confidence'],
                        'context': text
                    })
        
        return relations
    
    def _predict_relation(self, text):
        """é¢„æµ‹ä¸¤ä¸ªå®ä½“é—´çš„å…³ç³»"""
        inputs = self.tokenizer(
            text,
            return_tensors="pt",
            truncation=True,
            max_length=512
        )
        
        with torch.no_grad():
            outputs = self.model(**inputs)
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)[0]
            
            max_prob, max_idx = torch.max(probs, dim=0)
            
            return {
                'type': self.relation_types[max_idx],
                'confidence': float(max_prob)
            }
```

### 4.3 å®ä½“é“¾æ¥

```python
# ai_service/extraction/entity_linker.py
class EntityLinker:
    def __init__(self, embedding_model, existing_entities):
        self.embedding_model = embedding_model
        self.existing_entities = existing_entities
        self.entity_embeddings = self._compute_embeddings()
    
    def _compute_embeddings(self):
        """è®¡ç®—ç°æœ‰å®ä½“çš„åµŒå…¥"""
        embeddings = {}
        
        for entity_id, entity in self.existing_entities.items():
            # ä½¿ç”¨BERTæˆ–å…¶ä»–æ¨¡å‹è®¡ç®—æ–‡æœ¬åµŒå…¥
            text = entity['label'] + ' ' + entity.get('description', '')
            embedding = self.embedding_model.encode(text)
            embeddings[entity_id] = embedding
        
        return embeddings
    
    def link(self, extracted_entity, threshold=0.8):
        """å°†æŠ½å–çš„å®ä½“é“¾æ¥åˆ°çŸ¥è¯†å›¾è°±ä¸­çš„å®ä½“"""
        
        # è®¡ç®—æŠ½å–å®ä½“çš„åµŒå…¥
        entity_embedding = self.embedding_model.encode(
            extracted_entity['text']
        )
        
        # è®¡ç®—ä¸æ‰€æœ‰ç°æœ‰å®ä½“çš„ç›¸ä¼¼åº¦
        similarities = []
        
        for entity_id, existing_embedding in self.entity_embeddings.items():
            similarity = cosine_similarity(entity_embedding, existing_embedding)
            
            if similarity > threshold:
                similarities.append({
                    'entity_id': entity_id,
                    'similarity': float(similarity),
                    'entity': self.existing_entities[entity_id]
                })
        
        # æ’åº
        similarities.sort(key=lambda x: x['similarity'], reverse=True)
        
        return similarities[:5]  # Top 5å€™é€‰

def cosine_similarity(a, b):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦"""
    import numpy as np
    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b))
```

---

## ğŸ“‹ APIæ¥å£æ±‡æ€»

```python
# å›¾åµŒå…¥
POST /api/ai/embedding/train
Request: { graph: {...}, algorithm: "node2vec", params: {...} }
Response: { model_id: "xxx", metrics: {...} }

POST /api/ai/embedding/query
Request: { node_id: "epic_001", top_k: 10 }
Response: { similar_nodes: [...] }

# è‡ªç„¶è¯­è¨€æŸ¥è¯¢
POST /api/ai/nl-query/convert
Request: { question: "æ‰¾å‡ºæ‰€æœ‰é«˜ä¼˜å…ˆçº§çš„Epic", schema: {...} }
Response: { sparql: "SELECT ...", confidence: 0.9 }

POST /api/ai/qa/ask
Request: { question: "...", context: {...} }
Response: { answer: "...", sparql: "...", confidence: 0.8 }

# æ¨è
POST /api/ai/recommend/relations
Request: { source: "A", target: "B" }
Response: { predictions: [{ relation: "depends_on", confidence: 0.85 }] }

POST /api/ai/recommend/complete
Response: { suggestions: [{ source, target, relation, confidence }] }

# çŸ¥è¯†æŠ½å–
POST /api/ai/extract/entities
Request: { text: "..." }
Response: { entities: [...] }

POST /api/ai/extract/relations
Request: { text: "...", entities: [...] }
Response: { relations: [...] }

POST /api/ai/extract/link
Request: { entity: {...} }
Response: { candidates: [...] }
```

---

## ğŸ“Š éƒ¨ç½²æ–¹æ¡ˆ

### Dockerå®¹å™¨åŒ–

```dockerfile
# ai_service/Dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# ä¸‹è½½æ¨¡å‹
RUN python -m spacy download zh_core_web_sm

# å¤åˆ¶ä»£ç 
COPY . .

# å¯åŠ¨æœåŠ¡
CMD ["gunicorn", "-w", "4", "-b", "0.0.0.0:5000", "app:app"]
```

### ä¾èµ–ç®¡ç†

```txt
# requirements.txt
flask==2.3.0
flask-cors==4.0.0
numpy==1.24.0
torch==2.0.0
torch-geometric==2.3.0
transformers==4.30.0
spacy==3.5.0
networkx==3.1
gensim==4.3.0
faiss-cpu==1.7.4
langchain==0.0.200
openai==0.27.0
scikit-learn==1.2.2
umap-learn==0.5.3
```

---

## ğŸ“‹ äº¤ä»˜æ¸…å•

### Sprint 06: å›¾åµŒå…¥
- [ ] Python AIæœåŠ¡æ­å»º
- [ ] Node2Vecå®ç°
- [ ] TransEå®ç°
- [ ] FAISSå‘é‡å­˜å‚¨
- [ ] ç›¸ä¼¼åº¦æœç´¢API
- [ ] å‘é‡å¯è§†åŒ–

### Sprint 07: NLæŸ¥è¯¢
- [ ] LLMé›†æˆ
- [ ] NLâ†’SPARQLè½¬æ¢
- [ ] é—®ç­”ç³»ç»Ÿ
- [ ] å¯¹è¯ç®¡ç†
- [ ] æŸ¥è¯¢ç•Œé¢

### Sprint 08: æ¨è
- [ ] å…³ç³»é¢„æµ‹æ¨¡å‹
- [ ] çŸ¥è¯†è¡¥å…¨ç®—æ³•
- [ ] æ¨èAPI
- [ ] è¯„ä¼°æŠ¥å‘Š

### Sprint 09: çŸ¥è¯†æŠ½å–
- [ ] NERæ¨¡å‹é›†æˆ
- [ ] å…³ç³»æŠ½å–
- [ ] å®ä½“é“¾æ¥
- [ ] æ‰¹é‡å¤„ç†

---

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2026-01-16  
**çŠ¶æ€**: âœ… å°±ç»ª
